{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaH6MSaE3BFo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from typing import List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "# tweets = list(df_tweets.iloc[:, 1])  # fetch the tweets in a list form from the \"TWEETS\" feature of df_tweets \n",
        "\n",
        "@dataclass\n",
        "class DegreeOfProfanity:\n",
        "    \"\"\"This class shall be used for the computation of `Degree of Profanity` for each tweet in the given\n",
        "    list of tweets ~ \"tweets\".\n",
        "\n",
        "    Args:\n",
        "        tweets (List): List whose each element represents a tweet made by a certain user in the `str` format.\n",
        "        racial_slurs (List): A custom list in which each element is a racial slur/profane word based on which \n",
        "        the degree of profanity has to be computed for each tweet.\n",
        "    \"\"\"\n",
        "    tweets: List\n",
        "    racial_slurs: List\n",
        "\n",
        "    def _get_degree_of_profanity(self, tweet: List) -> float:\n",
        "        \"\"\"Computes and returns the `Degree of Profanity` for the given tweet.\n",
        "        Formula for computing `Degree of Profanity`:\n",
        "            Degree of Profanity = (Number of Profane Words in the tweet)/(total number of words in the tweet)\n",
        "\n",
        "        Args:\n",
        "            tweet (List): Tweet in word tokenized form for which the degree of profanity has to be computed.\n",
        "\n",
        "        Raises:\n",
        "            e: Raises an exception shoud any pops up while execution of this method.\n",
        "\n",
        "        Returns:\n",
        "            float: Output value of Degree of Profanity for the given tweet.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            degree_of_profanity = len([word for word in tweet if word in self.racial_slurs])/len(tweet)\n",
        "            return degree_of_profanity\n",
        "            ...\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "\n",
        "    def compute(self) -> List:\n",
        "        \"\"\"Parses each tweet in the given list of tweets, tokenizes them accompanied by their cleaning --\n",
        "         removal of stop words and lemmatization --, followed by calculation of degree of profanity for each \n",
        "         tweet and finally returns a list containing degree of profanity for each tweet.\n",
        "\n",
        "        Raises:\n",
        "            e: Raises an exception shoud any pops up while execution of this method.\n",
        "\n",
        "        Returns:\n",
        "            List: List whose each element represents the degree of profanity for each tweet. \n",
        "        \"\"\"\n",
        "        try:\n",
        "            lemma = WordNetLemmatizer()  # Lemmatizer object\n",
        "            degree_of_profanities = []  # List that will contain `Degree of Profanity` for each tweet\n",
        "            \n",
        "            for tweet in self.tweets:  # Parsing a tweet at a time\n",
        "                sentences = nltk.sent_tokenize(tweet)  # Sentence Tokenization\n",
        "                for sentence in sentences:\n",
        "                    words = nltk.words_tokenize(sentence)  # Word Tokenization\n",
        "                    words = words.lower()  # Lowering the words to obliterate the case sensitivity\n",
        "                    cleaned_tweet = [lemma.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
        "                    degree_of_profanities.append(self._get_degree_of_profanity(tweet=cleaned_tweet))\n",
        "            \n",
        "            return degree_of_profanities\n",
        "            ...\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "\n",
        "\n",
        "# Now, the degree of profanities can be assigned to the df_tweets accordingly.\n",
        "# degree_of_profanity = DegreeOfProfanity(tweets=tweets, racial_slurs=racial_slurs)\n",
        "# tweets_dop = degree_of_profanity.compute()\n",
        "# df_tweets[\"Degree of Profanity\"] = tweets_dop"
      ]
    }
  ]
}